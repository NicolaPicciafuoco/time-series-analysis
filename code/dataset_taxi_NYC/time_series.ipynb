{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Time Series Analysis of NYC Yellow Taxi Trips",
   "id": "c49c2a6fa96d0666"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install kagglehub\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install seaborn\n",
    "%pip install statsmodels\n",
    "%pip install pmdarima"
   ],
   "id": "770160370fe19f57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "from pmdarima import auto_arima  # Per ottimizzazione automatica dei parametri"
   ],
   "id": "e44cab84b86065d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset loading",
   "id": "1e1f46007a19354c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path = kagglehub.dataset_download(\"elemento/nyc-yellow-taxi-trip-data\")\n",
    "file_path1 = os.path.join(path, \"yellow_tripdata_2015-01.csv\")\n",
    "file_path2 = os.path.join(path, \"yellow_tripdata_2016-01.csv\")\n",
    "file_path3 = os.path.join(path, \"yellow_tripdata_2016-02.csv\")\n",
    "file_path4 = os.path.join(path, \"yellow_tripdata_2016-03.csv\")\n",
    "\n",
    "file_paths = [\n",
    "    #file_path1,\n",
    "    file_path2,\n",
    "    file_path3,\n",
    "    file_path4]\n"
   ],
   "id": "9a572f27766e4304",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Escludiamo il primo file CSV dal momento che esso è relativo al gennaio 2015, mostrando una separazione di numerosi mesi con gli altri file CSV, cosa che porterebbe a dei problemi nella strutturazione e nell'analisi della serie temporale risultante.",
   "id": "2db34b099fb9d0a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Columns of interest\n",
    "columns = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'fare_amount', 'total_amount']\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "dfs = [pd.read_csv(f, usecols=columns, parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']) for f in file_paths]\n",
    "df = pd.concat(dfs, ignore_index=True)"
   ],
   "id": "99f9c6723bd9cd6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data cleaning",
   "id": "f6d3538973ff33d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = df[(df['trip_distance'] > 0) & (df['fare_amount'] > 0) & (df['total_amount'] > 0)]",
   "id": "81289552a1e2a23d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df['pickup_date'] = df['tpep_pickup_datetime'].dt.date\n",
    "df['trip_duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "df = df[(df['trip_duration'] > 0) & (df['trip_duration'] < 240)]"
   ],
   "id": "fb37d0347921c717",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Creiamo due nuove colonne utilizzate, una come variabile temporale, ed entrambe per escludere altri record particolari (ulteriore data cleaning)",
   "id": "d474e53004b550c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Stationarity analysis",
   "id": "418529762a417313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "daily_trips = df.groupby('pickup_date').size()\n",
    "\n",
    "ts_daily_trips = daily_trips.copy()\n",
    "ts_daily_trips.index = pd.to_datetime(ts_daily_trips.index)\n",
    "ts_daily_trips = ts_daily_trips.asfreq('D', fill_value=0)"
   ],
   "id": "767a5cc3b4ac67e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ts_daily_trips['2016-01-23'] = ts_daily_trips[ts_daily_trips.index.dayofweek == 5].mean()\n",
    "ts_daily_trips['2016-01-24'] = ts_daily_trips[ts_daily_trips.index.dayofweek == 6].mean()\n",
    "ts_daily_trips['2016-01-25'] = ts_daily_trips[ts_daily_trips.index.dayofweek == 0].mean()\n",
    "ts_daily_trips['2016-01-26'] = ts_daily_trips[ts_daily_trips.index.dayofweek == 1].mean()"
   ],
   "id": "db1e8e11b40d055c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Questo \"riempimento\" con le medie è necessario perché nei giorni \"riempiti\" erano presenti dei valori mancanti, cosa che può causare problemi in una serie temporale. L'approccio di riempimento basato sulla media è stato approvato e va bene così.",
   "id": "e3b8361758c38fb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train and test\n",
    "train_size = int(len(ts_daily_trips) * 0.8)\n",
    "train_ts = ts_daily_trips[:train_size]\n",
    "test_ts = ts_daily_trips[train_size:]"
   ],
   "id": "a59b88c89982e73d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "La serie temporale è stata splittata in una parte di training e una di test, in modo tale da poter valutare le predizioni effettuate dai modelli che si costruiranno sulla base dell'andamento reale.",
   "id": "343b87bf36bcd47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "ts_daily_trips.plot()\n",
    "plt.title('Daily NYC Taxi Trips')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Trips')\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "8d8e6e7209a2e1d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test_stationarity_with_interpretation(timeseries):\n",
    "    result = adfuller(timeseries)\n",
    "    print('=== Augmented Dickey-Fuller Test ===')\n",
    "    print(f'ADF Statistic: {result[0]:.4f}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'   {key}: {value:.4f}')\n",
    "    \n",
    "    # Interpretazione del risultato\n",
    "    if result[1] <= 0.05:\n",
    "        print(\"\\n✅ La serie è stazionaria\")\n",
    "    else:\n",
    "        print(\"\\n❌ La serie NON è stazionaria\")\n",
    "\n",
    "# Test sulla serie originale\n",
    "print(\"\\n[TEST SULLA SERIE ORIGINALE]\")\n",
    "test_stationarity_with_interpretation(train_ts)"
   ],
   "id": "6620b3180cc3922e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The $p$-value is less than $0.05$, which means that we can reject the null hypothesis that the time series is non-stationary. The time series is stationary.",
   "id": "db3a8ba06ae16de1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Automatic optimization of parameters with auto_arima",
   "id": "795dd4bcb2d52a4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "auto_arima_model = auto_arima(train_ts, d=0, seasonal=False, trace=True, error_action='ignore', suppress_warnings=True)\n",
    "print(\"Best ARIMA model:\", auto_arima_model.summary())"
   ],
   "id": "1c85f54b80f1ab7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "auto_sarima_model = auto_arima(train_ts, d=0, seasonal=True, m=7, trace=True, error_action='ignore', suppress_warnings=True)\n",
    "print(\"Best SARIMA model:\", auto_sarima_model.summary())"
   ],
   "id": "f4a0b83e2d160366",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The auto_arima function calls, have shown that the best parameters are:\n",
    "- 3, 0, 3 for ARIMA\n",
    "- 1, 0, 1, 2, 1, 1, 7 for SARIMA\n",
    "\n",
    "However, those results does not reflect the choice of not differentiating the series (also, the results with those parameters were not so good). So, we will choose the parameters using the ACF and PACF plots."
   ],
   "id": "89b42e6b091a687a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ARIMA-SARIMA models\n",
    "### p, d, q parameters"
   ],
   "id": "cbb39161f6cb1fe0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plot_acf(train_ts, ax=plt.gca(), lags=20)\n",
    "plt.subplot(122)\n",
    "plot_pacf(train_ts, ax=plt.gca(), lags=20)\n",
    "plt.show()"
   ],
   "id": "e43465fc70519b94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "La struttura dei grafici ACF e PACF, unitamente all’analisi preliminare sulla stazionarietà della serie, suggerisce la presenza di componenti autoregressive e di media mobile di basso ordine. Nel dettaglio, il significativo picco della PACF al primo lag indica che un termine autoregressivo di ordine uno (p=1) è adeguato per catturare la dipendenza immediata della serie dai propri valori passati. Allo stesso tempo, l’ACF che decresce gradualmente verso zero ma non fornisce un taglio netto suggerisce l’opportunità di inserire un piccolo termine MA, portando all’impiego di un modello ARIMA(1,0,1) non stagionale.\n",
    "\n",
    "Poiché la serie presenta una chiara stagionalità settimanale (un ciclo di 7 giorni), si è deciso di estendere la struttura ARIMA a SARIMA, introducendo una componente stagionale con periodo s=7. L’analisi dei picchi su ACF e PACF ai multipli di 7 lag consente di discernere se inserire un termine AR o MA stagionale. L’individuazione di un significativo spike nella PACF a lag stagionali può guidare verso l’introduzione di un termine AR stagionale (P=1), mentre un picco di autocorrelazione stagionale nell’ACF motiva l’inclusione di un termine MA stagionale (Q=1). Nel caso in esame, l’ipotesi iniziale è di partire da un modello SARIMA(1,0,1)(1,0,0)[7], eventualmente valutando l’aggiunta di un termine MA stagionale se i residui mostrano ancora autocorrelazione sistematica."
   ],
   "id": "791ecaff4fd9a439"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = ARIMA(train_ts, order=(1, 0, 1))\n",
    "\n",
    "arima_fit = model.fit()\n",
    "\n",
    "print(\"\\n=== ARIMA Model Summary ===\")\n",
    "print(arima_fit.summary())"
   ],
   "id": "995e3d7a464375ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Risultato del training:\n",
    "\"\"\"\n",
    "=== ARIMA Model Summary ===\n",
    "                               SARIMAX Results\n",
    "==============================================================================\n",
    "Dep. Variable:                      y   No. Observations:                   72\n",
    "Model:                 ARIMA(1, 0, 1)   Log Likelihood                -828.204\n",
    "Date:                Sun, 08 Dec 2024   AIC                           1664.408\n",
    "Time:                        11:19:36   BIC                           1673.514\n",
    "Sample:                    01-01-2016   HQIC                          1668.033\n",
    "                         - 03-12-2016\n",
    "Covariance Type:                  opg\n",
    "==============================================================================\n",
    "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const       3.825e+05   7924.222     48.264      0.000    3.67e+05    3.98e+05\n",
    "ar.L1          0.3719      0.159      2.332      0.020       0.059       0.685\n",
    "ma.L1          0.6023      0.184      3.270      0.001       0.241       0.963\n",
    "sigma2      5.748e+08      0.154   3.72e+09      0.000    5.75e+08    5.75e+08\n",
    "===================================================================================\n",
    "Ljung-Box (L1) (Q):                   0.10   Jarque-Bera (JB):                 4.94\n",
    "Prob(Q):                              0.75   Prob(JB):                         0.08\n",
    "Heteroskedasticity (H):               1.19   Skew:                            -0.60\n",
    "Prob(H) (two-sided):                  0.68   Kurtosis:                         2.55\n",
    "===================================================================================\n",
    "\"\"\""
   ],
   "id": "189f29f1a1930396",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Diagnostic plots\n",
    "arima_fit.plot_diagnostics(figsize=(12,8))\n",
    "plt.show()"
   ],
   "id": "e8cb4195f6f8bde6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_sarima = sm.tsa.statespace.SARIMAX(train_ts, order=(1, 0, 1), seasonal_order=(1, 0, 0, 7))\n",
    "sarima_fit = model_sarima.fit(method='bfgs')\n",
    "\n",
    "print(sarima_fit.summary())"
   ],
   "id": "e51a41f0332c0244",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Risultati del training:\n",
    "\"\"\"\n",
    "SARIMAX Results\n",
    "==========================================================================================\n",
    "Dep. Variable:                                  y   No. Observations:                   72\n",
    "Model:             SARIMAX(1, 0, 1)x(1, 0, [], 7)   Log Likelihood                -824.441\n",
    "Date:                            Sun, 08 Dec 2024   AIC                           1656.882\n",
    "Time:                                    12:26:02   BIC                           1665.989\n",
    "Sample:                                01-01-2016   HQIC                          1660.508\n",
    "                                     - 03-12-2016\n",
    "Covariance Type:                              opg\n",
    "==============================================================================\n",
    "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "ar.L1          0.9826      0.043     22.837      0.000       0.898       1.067\n",
    "ma.L1         -0.3073      0.340     -0.905      0.365      -0.973       0.358\n",
    "ar.S.L7        0.7655      0.188      4.078      0.000       0.398       1.133\n",
    "sigma2      8.156e+08   4.74e-11   1.72e+19      0.000    8.16e+08    8.16e+08\n",
    "===================================================================================\n",
    "Ljung-Box (L1) (Q):                   0.38   Jarque-Bera (JB):                 0.64\n",
    "Prob(Q):                              0.54   Prob(JB):                         0.73\n",
    "Heteroskedasticity (H):               0.64   Skew:                             0.21\n",
    "Prob(H) (two-sided):                  0.28   Kurtosis:                         2.81\n",
    "===================================================================================\n",
    "\"\"\""
   ],
   "id": "bb3db902175f4df0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sarima_fit.plot_diagnostics(figsize=(12, 8))\n",
    "plt.show()"
   ],
   "id": "296f7de7171b9b64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "forecast_arima = arima_fit.get_forecast(steps=20)\n",
    "forecast_sarima = sarima_fit.get_forecast(steps=20)\n",
    "\n",
    "results = {\n",
    "    'ARIMA': (forecast_arima.predicted_mean, forecast_arima.se_mean, forecast_arima.conf_int()),\n",
    "    'SARIMA': (forecast_sarima.predicted_mean, forecast_sarima.se_mean, forecast_sarima.conf_int())\n",
    "}\n",
    "\n",
    "arima_fc_series = pd.Series(results['ARIMA'][0], index=test_ts.index)\n",
    "sarima_fc_series = pd.Series(results['SARIMA'][0], index=test_ts.index)\n",
    "\n",
    "arima_lower_series = pd.Series(results['ARIMA'][2].iloc[:, 0], index=test_ts.index)\n",
    "arima_upper_series = pd.Series(results['ARIMA'][2].iloc[:, 1], index=test_ts.index)\n",
    "\n",
    "sarima_lower_series = pd.Series(results['SARIMA'][2].iloc[:, 0], index=test_ts.index)\n",
    "sarima_upper_series = pd.Series(results['SARIMA'][2].iloc[:, 1], index=test_ts.index)\n",
    "\n",
    "plt.figure(figsize=(14, 7), dpi=300)\n",
    "plt.plot(train_ts, label='Training')\n",
    "plt.plot(test_ts, label='Test')\n",
    "plt.plot(arima_fc_series, label='ARIMA Forecast', color='red')\n",
    "plt.plot(sarima_fc_series, label='SARIMA Forecast', color='green')\n",
    "plt.ylim(200000, 600000)\n",
    "plt.fill_between(arima_lower_series.index, arima_lower_series, arima_upper_series, color='red', alpha=0.3)\n",
    "plt.fill_between(sarima_lower_series.index, sarima_lower_series, sarima_upper_series, color='green', alpha=0.3)\n",
    "plt.title('Forecast Comparison')\n",
    "plt.legend(loc='upper left', fontsize=10)\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "3c75a1da83f26f39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I risultati mostrano un'ottima predizione da parte del modello SARIMA(1, 0, 1)(1, 0, 0, 7), che si avvicina di molto all'andamento reale del dataset di test, nonostante gli intervalli di confidenza che, via via, aumentano esponenzialmente, non rappresentando un ottimo intervallo. Il modello ARIMA(1, 0, 1), invece, mostra una andamento che, dopo circa 3 step, si assesta su un andamento costante, mostrando un risultato pessimo.",
   "id": "5264280410ad5379"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hour based analysis\n",
    "\n",
    "Seconda analisi di serie temporale: dopo aver effettuato la prima analisi sul dataset trasformato in serie temporale ma sulla base di parametri giornalieri, proviamo ad effettuare un'analisi su misurazioni orarie. In questo modo dimostriamo di saper gestire serie temporali di tipologie diverse e valutiamo anche il comportamento dei modelli in condizioni operative differenti."
   ],
   "id": "e88ea5f6538f5fe5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Specificare i file rilevanti per gennaio, febbraio e marzo 2016\n",
    "hour_dfs = [pd.read_csv(f, usecols=columns, parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']) \n",
    "            for f in file_paths]\n",
    "hour_df = pd.concat(hour_dfs, ignore_index=True)"
   ],
   "id": "88d1ed18d4dd781b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Rimuovere righe con valori nulli\n",
    "hour_df.dropna(subset=columns, inplace=True)\n",
    "\n",
    "# Filtrare viaggi con valori negativi o anomali\n",
    "hour_df = hour_df[(hour_df['trip_distance'] > 0) & \n",
    "                  (hour_df['fare_amount'] > 0) & \n",
    "                  (hour_df['total_amount'] > 0)]\n",
    "\n",
    "# Calcolare la durata del viaggio\n",
    "hour_df['trip_duration'] = (hour_df['tpep_dropoff_datetime'] - hour_df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "# Filtrare viaggi con durata negativa o eccessiva (>4 ore)\n",
    "hour_df = hour_df[(hour_df['trip_duration'] > 0) & (hour_df['trip_duration'] < 240)]"
   ],
   "id": "a48c1663885b3c3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Stessa ETL effettuata precedentemente",
   "id": "be56b4cc9968c47b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creare un indice temporale arrotondando al più vicino inizio dell'ora\n",
    "hour_df['pickup_hour'] = hour_df['tpep_pickup_datetime'].dt.floor('H')\n",
    "\n",
    "# Aggregare il numero di viaggi per ogni ora\n",
    "hourly_trips = hour_df.groupby('pickup_hour').size()\n",
    "\n",
    "# Trasformare in una serie temporale con frequenza fissa\n",
    "hourly_ts = hourly_trips.asfreq('H', fill_value=0)\n",
    "\n",
    "# Fill all the hours of January 23-24-25-26, 2016 with the average of the corresponding hours of the corresponding days-of-week in the rest of the dataset\n",
    "# NOT the average of the corresponding days-of-week, but the average of the corresponding hours\n",
    "# Riempimento delle ore mancanti basato su medie dello stesso giorno della settimana\n",
    "for i in range(24):\n",
    "    hourly_ts[f'2016-01-23 {i:02d}:00:00'] = hourly_ts[\n",
    "        (hourly_ts.index.hour == i) & (hourly_ts.index.dayofweek == 5)\n",
    "    ].mean()\n",
    "    hourly_ts[f'2016-01-24 {i:02d}:00:00'] = hourly_ts[\n",
    "        (hourly_ts.index.hour == i) & (hourly_ts.index.dayofweek == 6)\n",
    "    ].mean()\n",
    "\n",
    "\n",
    "# Train and test\n",
    "train_size = int(len(hourly_ts) * 0.8)\n",
    "train_hourly_ts = hourly_ts[:train_size]\n",
    "test_hourly_ts = hourly_ts[train_size:]"
   ],
   "id": "d7f9d3f60462b0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Stesse operazioni effettuate precedentemente ma su base oraria. Anche il riempimento dei valori mancanti è stato effettuato per gli stessi motivi visti precedentemente",
   "id": "4bc5a4f8c603db59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "train_hourly_ts.plot()\n",
    "plt.title('Numero di viaggi per ora (gennaio-febbraio 2016)')\n",
    "plt.xlabel('Ora')\n",
    "plt.ylabel('Numero di viaggi')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ],
   "id": "be5389aa62715905",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n[TEST SULLA SERIE ORIGINALE]\")\n",
    "test_stationarity_with_interpretation(train_hourly_ts)"
   ],
   "id": "d1b798ff7d7ced25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "La serie è stazionaria: non serve differenziarla",
   "id": "4ed1c8d6baeca814"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "acf_values = acf(train_hourly_ts, nlags=30)\n",
    "pacf_values = pacf(train_hourly_ts, nlags=30)\n",
    "print(\"ACF:\", acf_values)\n",
    "print(\"PACF:\", pacf_values)"
   ],
   "id": "84d59d1c26a4207d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Valori \"printati\" di ACF e PACF, in modo da poter capire ancora meglio rispetto ai grafici sottostanti l'andamento e la scelta dei parametri dei modelli",
   "id": "80d110fb6b003bc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plot_acf(train_hourly_ts, ax=plt.gca(), lags=20)\n",
    "plt.subplot(122)\n",
    "plot_pacf(train_hourly_ts, ax=plt.gca(), lags=20)\n",
    "plt.show()"
   ],
   "id": "f0fc4c8d4aa39a2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Osservando i picchi di autocorrelazione a lag ravvicinati e l’evidente stagionalità giornaliera (24 ore), è stato selezionato un modello SARIMA in grado di catturare sia la componente non stagionale che quella stagionale a 24 ore. La presenza di un forte picco all’ACF sul primo lag e la struttura residuale in PACF suggeriscono un termine AR e un termine MA non stagionali di basso ordine (ad esempio ARIMA(1,0,1)). L’inclusione di componenti stagionali AR e MA sul periodo di 24 ore, come SARIMA(1,0,1)(1,0,1,24), consente di modellare efficacemente il pattern ripetitivo giornaliero.",
   "id": "891eceb7b0552f5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hourly_ts_log = np.log1p(train_hourly_ts)\n",
    "\n",
    "hourly_arima_model = ARIMA(hourly_ts_log, order=(1, 0, 1))\n",
    "\n",
    "hourly_arima_fit = hourly_arima_model.fit()\n",
    "\n",
    "print(\"\\n=== ARIMA Model Summary ===\")\n",
    "print(hourly_arima_fit.summary())"
   ],
   "id": "56922e09128ae009",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "NOTA: in questo caso abbiamo effettuato una normalizzazione con np.log1p perché i valori erano troppo \"traballanti\"",
   "id": "b4b1113e26cc669d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Risultato del training:\n",
    "\"\"\"\n",
    "=== ARIMA Model Summary ===\n",
    "                               SARIMAX Results\n",
    "==============================================================================\n",
    "Dep. Variable:                      y   No. Observations:                 1747\n",
    "Model:                 ARIMA(1, 0, 1)   Log Likelihood              -16135.310\n",
    "Date:                Sun, 08 Dec 2024   AIC                          32278.621\n",
    "Time:                        12:27:01   BIC                          32300.484\n",
    "Sample:                    01-01-2016   HQIC                         32286.703\n",
    "                         - 03-13-2016\n",
    "Covariance Type:                  opg\n",
    "==============================================================================\n",
    "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "const       1.587e+04    662.957     23.940      0.000    1.46e+04    1.72e+04\n",
    "ar.L1          0.8457      0.016     52.695      0.000       0.814       0.877\n",
    "ma.L1          0.6122      0.008     75.517      0.000       0.596       0.628\n",
    "sigma2      6.154e+06   1.27e+05     48.528      0.000    5.91e+06     6.4e+06\n",
    "===================================================================================\n",
    "Ljung-Box (L1) (Q):                  55.24   Jarque-Bera (JB):             20484.33\n",
    "Prob(Q):                              0.00   Prob(JB):                         0.00\n",
    "Heteroskedasticity (H):               1.90   Skew:                             0.33\n",
    "Prob(H) (two-sided):                  0.00   Kurtosis:                        19.76\n",
    "===================================================================================\n",
    "\"\"\""
   ],
   "id": "d7244840361b4622",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hourly_arima_fit.plot_diagnostics(figsize=(12,8))\n",
    "plt.show()"
   ],
   "id": "1f1ac895195a0448",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Applicare la trasformazione logaritmica\n",
    "hourly_ts_log = np.log1p(train_hourly_ts)\n",
    "\n",
    "# Modello SARIMA\n",
    "hourly_model_sarima = sm.tsa.statespace.SARIMAX(hourly_ts_log, order=(1, 0, 1), seasonal_order=(1, 0, 1, 24))\n",
    "hourly_sarima_fit = hourly_model_sarima.fit()\n",
    "\n",
    "print(hourly_sarima_fit.summary())"
   ],
   "id": "bf2b881320d19b35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Risultato del training:\n",
    "\"\"\"\n",
    "                                     SARIMAX Results\n",
    "==========================================================================================\n",
    "Dep. Variable:                                  y   No. Observations:                 2184\n",
    "Model:             SARIMAX(1, 0, 1)x(1, 0, 1, 24)   Log Likelihood                -285.741\n",
    "Date:                            Sun, 08 Dec 2024   AIC                            581.481\n",
    "Time:                                    12:27:03   BIC                            609.926\n",
    "Sample:                                01-01-2016   HQIC                           591.879\n",
    "                                     - 03-31-2016\n",
    "Covariance Type:                              opg\n",
    "==============================================================================\n",
    "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
    "------------------------------------------------------------------------------\n",
    "ar.L1          0.9988      0.001    747.181      0.000       0.996       1.001\n",
    "ma.L1          0.0548      0.003     16.586      0.000       0.048       0.061\n",
    "ar.S.L24       0.9995      0.001   1487.106      0.000       0.998       1.001\n",
    "ma.S.L24      -0.9750      0.015    -64.387      0.000      -1.005      -0.945\n",
    "sigma2         0.0737      0.000    190.866      0.000       0.073       0.074\n",
    "===================================================================================\n",
    "Ljung-Box (L1) (Q):                   0.06   Jarque-Bera (JB):           8518933.29\n",
    "Prob(Q):                              0.81   Prob(JB):                         0.00\n",
    "Heteroskedasticity (H):               3.70   Skew:                             2.45\n",
    "Prob(H) (two-sided):                  0.00   Kurtosis:                       308.93\n",
    "===================================================================================\n",
    "\"\"\""
   ],
   "id": "b7852e7b07cff891",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hourly_sarima_fit.plot_diagnostics(figsize=(12, 8))\n",
    "plt.show()"
   ],
   "id": "f3038faaf7e7a19b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hourly_forecast_arima = hourly_arima_fit.get_forecast(steps=len(test_hourly_ts))\n",
    "hourly_forecast_sarima_log = hourly_sarima_fit.get_forecast(steps=len(test_hourly_ts))\n",
    "hourly_forecast_sarima = np.expm1(hourly_forecast_sarima_log.predicted_mean)\n",
    "hourly_forecast_sarima_se = hourly_forecast_sarima_log.se_mean\n",
    "hourly_forecast_sarima_ci = np.expm1(hourly_forecast_sarima_log.conf_int())\n",
    "\n",
    "# Allineare gli indici delle previsioni con il test set\n",
    "arima_fc_series = pd.Series(hourly_forecast_arima.predicted_mean.values, index=test_hourly_ts.index)\n",
    "sarima_fc_series = pd.Series(hourly_forecast_sarima.values, index=test_hourly_ts.index)\n",
    "\n",
    "arima_lower_series = pd.Series(hourly_forecast_arima.conf_int().iloc[:, 0].values, index=test_hourly_ts.index)\n",
    "arima_upper_series = pd.Series(hourly_forecast_arima.conf_int().iloc[:, 1].values, index=test_hourly_ts.index)\n",
    "\n",
    "sarima_lower_series = pd.Series(hourly_forecast_sarima_ci.iloc[:, 0].values, index=test_hourly_ts.index)\n",
    "sarima_upper_series = pd.Series(hourly_forecast_sarima_ci.iloc[:, 1].values, index=test_hourly_ts.index)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Calcolo delle metriche di errore\n",
    "arima_rmse = np.sqrt(mean_squared_error(test_hourly_ts, arima_fc_series))\n",
    "sarima_rmse = np.sqrt(mean_squared_error(test_hourly_ts, sarima_fc_series))\n",
    "arima_mae = mean_absolute_error(test_hourly_ts, arima_fc_series)\n",
    "sarima_mae = mean_absolute_error(test_hourly_ts, sarima_fc_series)\n",
    "\n",
    "print(f\"ARIMA RMSE: {arima_rmse:.2f}, MAE: {arima_mae:.2f}\")\n",
    "print(f\"SARIMA RMSE: {sarima_rmse:.2f}, MAE: {sarima_mae:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(14, 7), dpi=300)\n",
    "plt.plot(train_hourly_ts, label='Training')\n",
    "plt.plot(test_hourly_ts, label='Test')\n",
    "plt.plot(arima_fc_series, label='ARIMA Forecast', color='red')\n",
    "plt.plot(sarima_fc_series, label='SARIMA Forecast', color='green')\n",
    "plt.ylim(-1000, 35000)\n",
    "plt.fill_between(arima_lower_series.index, arima_lower_series, arima_upper_series, color='red', alpha=0.3)\n",
    "plt.fill_between(sarima_lower_series.index, sarima_lower_series, sarima_upper_series, color='green', alpha=0.3)\n",
    "plt.title(f'Forecast Comparison (ARIMA RMSE: {arima_rmse:.2f}, SARIMA RMSE: {sarima_rmse:.2f})')\n",
    "plt.legend(loc='upper left', fontsize=10)\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "35ff81173793ea20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I risultati della predizione mostrano un andamento costante pessimo per il modello ARIMA(1, 0, 1), mentre un andamento molto migliore per il modello SARIMA(1, 0, 1)(1, 0, 1, 24), che segue la stagionalità. Tuttavia, le predizioni del modello SARIMA, dopo qualche step, sembrano seguire un andamento che, via via, tende ad abbassarsi, mostrando come, a lungo andare, la predizione potrebbe essere pressoché sbagliata. Inoltre, i valori di RMSE risultano essere particolarmente non ottimali, essendo nell'ordine di grandezza delle migliaia (ARIMA RMSE=, SARIMA RMSE=, ARIMA MAE=, SARIMA MAE=).\n",
    "\n",
    "Per questo motivo, si è deciso di introdurre un'ulteriore stagionalità settimanale, continuando a considerare la stagionalità giornaliera. Per fare ciò, la serie è stata differenziata con parametro 24."
   ],
   "id": "8bf484ec8ee76856"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Differenziazione per la stagionalità giornaliera (24 ore)\n",
    "hourly_ts_diff = hourly_ts.diff(24).dropna()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plot_acf(hourly_ts_diff, ax=plt.gca(), lags=20)\n",
    "plt.subplot(122)\n",
    "plot_pacf(hourly_ts_diff, ax=plt.gca(), lags=20)\n",
    "plt.show()"
   ],
   "id": "d7b168fbfea102f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I grafici di autocorrelazione e autocorrelazione parziale seguono un andamento estremamente simile ai grafici visti nel caso precedente, quindi si sceglie un modello SARIMA con gli stessi parametri del precedente, ma con stagionalità di 24*7",
   "id": "74e7a829903e45a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Suddivisione del dataset in training e test (80/20)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(hourly_ts_diff) * split_ratio)\n",
    "train_hourly_ts = hourly_ts_diff[:split_index]\n",
    "test_hourly_ts = hourly_ts_diff[split_index:]\n",
    "\n",
    "# Modello SARIMA considerando la stagionalità settimanale\n",
    "sarima_model = sm.tsa.statespace.SARIMAX(train_hourly_ts,\n",
    "                                         order=(1, 0, 1),  # Parametri ARIMA\n",
    "                                         seasonal_order=(1, 0, 1, 24 * 7))  # Stagionalità settimanale\n",
    "sarima_fit = sarima_model.fit()\n",
    "\n",
    "# Previsioni per il set di test\n",
    "forecast_steps = len(test_hourly_ts)  # Lunghezza del set di test\n",
    "hourly_forecast_sarima = sarima_fit.get_forecast(steps=forecast_steps)\n",
    "forecast_mean = hourly_forecast_sarima.predicted_mean\n",
    "forecast_ci = hourly_forecast_sarima.conf_int()\n",
    "\n",
    "# Allineare gli indici delle previsioni con il test set\n",
    "sarima_fc_series = pd.Series(forecast_mean.values, index=test_hourly_ts.index)\n",
    "sarima_lower_series = pd.Series(forecast_ci.iloc[:, 0].values, index=test_hourly_ts.index)\n",
    "sarima_upper_series = pd.Series(forecast_ci.iloc[:, 1].values, index=test_hourly_ts.index)\n",
    "\n",
    "# Calcolo delle metriche di errore\n",
    "sarima_rmse = np.sqrt(mean_squared_error(test_hourly_ts, sarima_fc_series))\n",
    "sarima_mae = mean_absolute_error(test_hourly_ts, sarima_fc_series)\n",
    "\n",
    "print(f\"SARIMA RMSE: {sarima_rmse:.2f}, MAE: {sarima_mae:.2f}\")\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "plt.figure(figsize=(14, 7), dpi=300)\n",
    "plt.plot(train_hourly_ts, label='Training')\n",
    "plt.plot(test_hourly_ts, label='Test', color='blue')\n",
    "plt.plot(sarima_fc_series, label='SARIMA Forecast', color='green')\n",
    "plt.fill_between(sarima_lower_series.index, sarima_lower_series, sarima_upper_series, color='green', alpha=0.3)\n",
    "plt.title(f'SARIMA Forecast with Weekly Seasonality (RMSE: {sarima_rmse:.2f})')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.legend(loc='upper left', fontsize=10)\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "d797689bd638e598",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In questi risultati, SARIMA si comporta estremamente bene. Osservando, infatti, in corrispondenza del test dataset, si nota come l'andamento predetto da SARIMA segue molto bene l'andamento reale.",
   "id": "7eaa1a3003ff8b94"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
