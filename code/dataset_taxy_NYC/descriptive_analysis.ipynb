{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for statistical  and time series analysis of the dataset \"NYC Yellow Taxi Trip Data\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the required packages, run this cell only once (code 1)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install kagglehub\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install seaborn\n",
    "%pip install folium \n",
    "%pip install scipy\n",
    "%pip install -U jupyter ipywidgets\n",
    "%pip install -U jupyterlab-widgets\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm, alpha\n",
    "import numpy as np\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium.plugins import HeatMap, MarkerCluster\n",
    "from datetime import datetime\n",
    "from scipy.stats import probplot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset and load the data\n",
    "##### run this cell only once (code 2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"elemento/nyc-yellow-taxi-trip-data\")\n",
    "# print(\"Path to dataset files:\", path) use only for debug\n",
    "file_path1 = os.path.join(path, \"yellow_tripdata_2015-01.csv\")  # Path to the dataset file\n",
    "file_path2 = os.path.join(path, \"yellow_tripdata_2016-01.csv\")  # Path to the dataset file\n",
    "file_path3 = os.path.join(path, \"yellow_tripdata_2016-02.csv\")  # Path to the dataset file\n",
    "file_path4 = os.path.join(path, \"yellow_tripdata_2016-03.csv\")  # Path to the dataset file"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### there is an error in one column, so you must run this script before to update and change the column name in the first dataset (if you need it)  (code 3)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def update_and_change(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Update the column name\n",
    "    if 'RateCodeID' in df.columns:\n",
    "        df.rename(columns={'RateCodeID': 'RatecodeID'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "df_2015_01 = update_and_change(file_path1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Check if database was imported correctly, this script is optional and it'll be used only for debugging (code 4)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check if the file exists\n",
    "if os.path.exists(file_path1):\n",
    "    # Upload the file to the notebook\n",
    "    df = pd.read_csv(file_path1)\n",
    "\n",
    "    # Print the colum names\n",
    "    print(\"Column name in the dataset:\", df_2015_01.columns.tolist())\n",
    "else:\n",
    "    print(f\"File not found. Check if the path is correct: {file_path1}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical analysis in colum VendorID for the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Union of the dataset and creation of the bar plot for the frequency of the VendorID (code 5)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# !before run this you must run the code 1 and 2\n",
    "vendor_colum = ['VendorID']\n",
    "df_2015_01 = pd.read_csv(file_path1, usecols=vendor_colum)    # Load the dataset  in a DataFrame\n",
    "df_2016_01 = pd.read_csv(file_path2, usecols=vendor_colum)    # Load the dataset  in a DataFrame\n",
    "df_2016_02 = pd.read_csv(file_path3, usecols=vendor_colum)    # Load the dataset  in a DataFrame\n",
    "df_2016_03 = pd.read_csv(file_path4, usecols=vendor_colum)    # Load the dataset  in a DataFrame\n",
    "print(\"Number of rows in the dataset 2015-01:\", len(df_2015_01))\n",
    "print(\"Number of rows in the dataset 2016-01:\", len(df_2016_01))\n",
    "print(\"Number of rows in the dataset 2016-02:\", len(df_2016_02))\n",
    "print(\"Number of rows in the dataset 2016-03:\", len(df_2016_03))\n",
    "\n",
    "# Concatenate the dataframes\n",
    "df = pd.concat([df_2015_01, df_2016_01, df_2016_02, df_2016_03], ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of the bar plot  and pie chart for the frequency of the VendorID (code 6)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Set a default style \n",
    "plt.style.use('default')\n",
    "\n",
    "# Calculate frequency of \n",
    "fr_vendor = df['VendorID'].value_counts()\n",
    "\n",
    "# Create bar plot with notation and color background\n",
    "fig, ax = plt.subplots()\n",
    "fig.patch.set_facecolor('white')  # set colour background\n",
    "ax.set_facecolor('white')  # set graphic colour\n",
    "\n",
    "# Bar plot\n",
    "fr_vendor.plot(kind='bar', color=['#1f77b4', '#2ca02c'], ax=ax)\n",
    "\n",
    "# Notation for each bar\n",
    "for i, count in enumerate(fr_vendor):\n",
    "    ax.text(i, count + 5, str(count), ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Title and labels\n",
    "ax.set_title(\"Frequency of VendorID\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"VendorID\", fontsize=12)\n",
    "ax.set_ylabel(\"Number of trips\", fontsize=12)\n",
    "ax.set_xticks(range(len(fr_vendor.index)))\n",
    "ax.set_xticklabels(fr_vendor.index, rotation=0)\n",
    "\n",
    "# Display chart\n",
    "plt.show()\n",
    "\n",
    "# Create pie chart with notation and color background\n",
    "plt.figure(figsize=(8, 8), facecolor='white')  # set colour background\n",
    "fr_vendor.plot(kind='pie', \n",
    "               autopct=lambda p: f'{p:.1f}% ({int(p * fr_vendor.sum() / 100)})',  # per cent\n",
    "               colors=['#1f77b4', '#2ca02c'], \n",
    "               startangle=90, \n",
    "               counterclock=False, \n",
    "               wedgeprops=dict(width=0.3))  # thickness\n",
    "\n",
    "# Title\n",
    "plt.title(\"Percentage of trips by VendorID\", fontsize=14, fontweight='bold')\n",
    "\n",
    "# Disply chart\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the distribution of the trips by hour (code 7)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# !before run this you must use code 1, 2 and 3\n",
    "# Union in a single csv only with the column RateCodeID\n",
    "colum_rate = ['RatecodeID'] \n",
    "df_2015_01_filtered = df_2015_01[['RatecodeID']]    # Load the dataset  in a DataFrame\n",
    "df_2016_01 = pd.read_csv(file_path2, usecols=colum_rate)    # Load the dataset  in a DataFrame\n",
    "df_2016_02 = pd.read_csv(file_path3, usecols=colum_rate)    # Load the dataset  in a DataFrame\n",
    "df_2016_03 = pd.read_csv(file_path4, usecols=colum_rate)    # Load the dataset  in a DataFrame\n",
    "df=pd.concat([df_2015_01_filtered, df_2016_01, df_2016_02, df_2016_03], ignore_index=True)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the bar plot for the frequency of the RateCodeID (code 8)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Creation of the bar plot for the frequency of the RateCodeID\n",
    "df['Validity_RateCodeID'] = df['RatecodeID'].apply(lambda x: 'valid' if x in range(1, 7) else 'not valid')\n",
    "\n",
    "# Filter the valid RateCodeID\n",
    "ratecode_validi = df[df['Validity_RateCodeID'] == 'valid']['RatecodeID']\n",
    "\n",
    "# Calculate the frequency of the RateCodeID\n",
    "frequent_rate_id = ratecode_validi.value_counts()\n",
    "frequent_rate_id['not valid'] = df['Validity_RateCodeID'].value_counts().get('not valid', 0)\n",
    "\n",
    "# Plot the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = frequent_rate_id.plot(kind='bar', color=['skyblue' if idx != 'not valid' else 'salmon' for idx in frequent_rate_id.index])\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Frequency of tariff code (RatecodeID)\")\n",
    "plt.xlabel(\"RatecodeID\")\n",
    "plt.ylabel(\"Number of ride\")\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Add annotations for each bar\n",
    "for i, count in enumerate(frequent_rate_id):\n",
    "    ax.text(i, count + max(frequent_rate_id) * 0.01, str(count), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Show the bar plot\n",
    "plt.show()\n",
    "\n",
    "# Add a pie chart for the % of the RateCodeID\n",
    "plt.figure(figsize=(8, 8), facecolor='white')  \n",
    "frequent_rate_id.plot(kind='pie', \n",
    "                      autopct=lambda p: f'{p:.1f}% ({int(p * frequent_rate_id.sum() / 100)})',\n",
    "                      colors=['skyblue' if idx != 'Non valido' else 'salmon' for idx in frequent_rate_id.index], \n",
    "                      startangle=90, \n",
    "                      counterclock=False, \n",
    "                      wedgeprops=dict(width=0.3))  \n",
    "\n",
    "plt.title(\"Percent of ride by RatecodeID\", fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(frequent_rate_id)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "colum = ['tpep_pickup_datetime'] \n",
    "df_2015_01_filtered = pd.read_csv(file_path1, usecols=colum) # Load the dataset in a DataFrame\n",
    "df_2016_01 = pd.read_csv(file_path2, usecols=colum)     \n",
    "df_2016_02 = pd.read_csv(file_path3, usecols=colum)\n",
    "df_2016_03 = pd.read_csv(file_path4, usecols=colum)\n",
    "\n",
    "df_total = pd.concat([df_2015_01_filtered, df_2016_01, df_2016_02, df_2016_03], ignore_index=True)\n",
    "\n",
    "# Convert the column to datetime\n",
    "df_total['tpep_pickup_datetime'] = pd.to_datetime(df_total['tpep_pickup_datetime'])\n",
    "\n",
    "# Extract the hour from the datetime\n",
    "df_total['hour'] = df_total['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "# Count the frequency of each hour\n",
    "hour_freq = df_total['hour'].value_counts().sort_index()\n",
    "\n",
    "# Create an array with the hours\n",
    "hours=hour_freq.index\n",
    "counts=hour_freq.values\n",
    "\n",
    "# Print frequency of each hour\n",
    "print(hour_freq)\n",
    "\n",
    "# Plot by hour distribution of trips\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=hours, y=counts, color='skyblue')\n",
    "plt.title(\"Distriubution of trips by hour\")\n",
    "plt.xlabel(\"hour\")\n",
    "plt.ylabel(\"Number of trips\")\n",
    "plt.xticks(np.arange(0, 24, 1))\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the distribution of the trips by hour in a day"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "colum = ['tpep_pickup_datetime']  # select column tpep_pickup_datetime\n",
    "df_2015_01_filtered = pd.read_csv(file_path1, usecols=colum)\n",
    "df_2016_01 = pd.read_csv(file_path2, usecols=colum)\n",
    "df_2016_02 = pd.read_csv(file_path3, usecols=colum)\n",
    "df_2016_03 = pd.read_csv(file_path4, usecols=colum)\n",
    "\n",
    "df_total = pd.concat([df_2015_01_filtered, df_2016_01, df_2016_02, df_2016_03], ignore_index=True)\n",
    "\n",
    "# Convert the column to datetime\n",
    "df_total['tpep_pickup_datetime'] = pd.to_datetime(df_total['tpep_pickup_datetime'])\n",
    "\n",
    "# Count the frequency of each day\n",
    "df_total['day'] = df_total['tpep_pickup_datetime'].dt.date\n",
    "\n",
    "# Count the frequency of each hour\n",
    "df_total['hour'] = df_total['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "hourly_day_freq = df_total.groupby(['day', 'hour']).size().reset_index(name='count')\n",
    "\n",
    "avg_hourly_by_day = hourly_day_freq.groupby('hour')['count'].mean().reset_index(name='avg_count')\n",
    "\n",
    "# Print average\n",
    "print(avg_hourly_by_day)\n",
    "\n",
    "# Plot chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=avg_hourly_by_day['hour'], y=avg_hourly_by_day['avg_count'], color='skyblue')\n",
    "plt.title(\"Distribution of Trips by hour in a day\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Average Number of Trips\")\n",
    "plt.xticks(np.arange(0, 24, 1))\n",
    "plt.show()\n",
    "\n",
    "# Print all days found in the dataset\n",
    "unique_days = df_total['day'].unique()\n",
    "unique_days_sorted = np.sort(unique_days)\n",
    "\n",
    "print(\"Days found in the dataset:\")\n",
    "for day in unique_days_sorted:\n",
    "    print(day)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to fit the distribution of the trips by hour with a Gaussian distribution (code 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "hour_freq = df_total['hour'].value_counts().sort_index()\n",
    "\n",
    "# 1. Standardizzazione dei dati\n",
    "mean = hour_freq.mean()  # Calculate the mean\n",
    "std = hour_freq.std()    # Deviazione standard for frequency of trips\n",
    "# This process can traform the data for have a media of 0 and a standard deviation of 1\n",
    "\n",
    "\n",
    "hour_freq_standardized = (hour_freq - mean) / std # Standardizzazione of the frequency of trips\n",
    "\n",
    "# 2. Q-Q Plot for check the normality of the data\n",
    "# Q-Q Plot (Quantile-Quantile Plot) compares the quantiles of your observed data (in this case, hour_freq_standardized) with the quantiles of a theoretical normal distribution.\n",
    "# If your data follows a normal distribution:\n",
    "# The Q-Q Plot points line up along the red diagonal line (representing the theoretical values of a standardized normal).\n",
    "# If the data deviate from the diagonal line:\n",
    "# Indicates that the data do not follow a normal distribution.\n",
    "# The further the points are from the diagonal line, the more the data deviate from a normal distribution.\n",
    "plt.figure(figsize=(10, 6))\n",
    "probplot(hour_freq_standardized, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q To check the normality of the data\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the mean and the standard deviation of the distribution of the trips by hour (code 9)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "mu, std = norm.fit(df_total['hour'])\n",
    "\n",
    "# create the gaussian distribution\n",
    "xmin,xmax=0,23\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x,p,'r-', lw=2 ,label='Gaussian fit' ) # Gaussian fit\n",
    "sns.histplot(df_total['hour'], bins=24, color='skyblue', edgecolor='black', stat='density', kde=False,alpha=0.6)\n",
    "#order the x axis and put the center of gayssian distribution in the middle\n",
    "plt.xlim(xmin,xmax)\n",
    "plt.title(\"Distribution of trips by hour\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xticks(np.arange(0, 24, 1))\n",
    "plt.show()\n",
    "# plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### barplot follow median and standard deviation (code 10)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#! before run this you must run the code 1 and 2\n",
    "colum = ['tpep_pickup_datetime'] # scelgo la colonna tpep_pickup_datetime\n",
    "df_2015_01_filtered = pd.read_csv(file_path1, usecols=colum)    # Load the dataset  in a DataFrame\n",
    "df_2016_01 = pd.read_csv(file_path2, usecols=colum)     \n",
    "df_2016_02 = pd.read_csv(file_path3, usecols=colum)\n",
    "df_2016_03 = pd.read_csv(file_path4, usecols=colum)\n",
    "\n",
    "df_total = pd.concat([df_2015_01_filtered, df_2016_01, df_2016_02, df_2016_03], ignore_index=True)\n",
    "\n",
    "# convert the column to datetime\n",
    "df_total['tpep_pickup_datetime'] = pd.to_datetime(df_total['tpep_pickup_datetime'])\n",
    "\n",
    "# extract the hour from the datetime\n",
    "df_total['hour'] = df_total['tpep_pickup_datetime'].dt.hour\n",
    "# Calcola la media e la deviazione standard\n",
    "mu = df_total['hour'].mean()\n",
    "std = df_total['hour'].std()\n",
    "\n",
    "# Definisce i limiti dell'asse x e calcola la distribuzione normale\n",
    "xmin, xmax = 0, 23\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "\n",
    "# Crea il grafico\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, p, 'r-', lw=2, label='Distribuzione Gaussiana')  # Curva gaussiana\n",
    "sns.histplot(df_total['hour'], bins=24, color='skyblue', edgecolor='black', stat='density', kde=False, alpha=0.6)\n",
    "\n",
    "# Aggiunge la media e la deviazione standard al grafico\n",
    "plt.axvline(mu, color='blue', linestyle='--', linewidth=1, label=f'Media: {mu:.2f}')\n",
    "plt.axvline(mu + std, color='green', linestyle='--', linewidth=1, label=f'Media + 1σ: {mu + std:.2f}')\n",
    "plt.axvline(mu - std, color='green', linestyle='--', linewidth=1, label=f'Media - 1σ: {mu - std:.2f}')\n",
    "\n",
    "# Aggiunge titolo e etichette\n",
    "plt.title(\"Distribuzione oraria delle corse dei taxi\")\n",
    "plt.xlabel(\"Ora del giorno\")\n",
    "plt.ylabel(\"Densità (frequenza normalizzata)\")\n",
    "plt.xticks(np.arange(0, 24, 1))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "colum = ['tpep_pickup_datetime'] # scelgo la colonna tpep_pickup_datetime\n",
    "df_2015_01_filtered = pd.read_csv(file_path1, usecols=colum)    # Load the dataset  in a DataFrame\n",
    "df_2016_01 = pd.read_csv(file_path2, usecols=colum)     \n",
    "df_2016_02 = pd.read_csv(file_path3, usecols=colum)\n",
    "df_2016_03 = pd.read_csv(file_path4, usecols=colum)\n",
    "\n",
    "df_total = pd.concat([df_2015_01_filtered, df_2016_01, df_2016_02, df_2016_03], ignore_index=True)\n",
    "\n",
    "# convert the column to datetime\n",
    "df_total['tpep_pickup_datetime'] = pd.to_datetime(df_total['tpep_pickup_datetime'])\n",
    "\n",
    "# extract the hour from the datetime\n",
    "df_total['hour'] = df_total['tpep_pickup_datetime'].dt.hour\n",
    "# Calcolare la media e la mediana delle ore\n",
    "mu_decimal = df_total['hour'].mean()\n",
    "median_hour = df_total['hour'].median()\n",
    "sigma = df_total['hour'].std()\n",
    "\n",
    "# Convertire la media decimale in formato HH:MM\n",
    "hours = int(mu_decimal)\n",
    "minutes = int((mu_decimal - hours) * 60)\n",
    "\n",
    "# Stampare media e mediana\n",
    "print(f\"La media delle ore è: {hours}:{minutes:02d}\")\n",
    "print(f\"La mediana delle ore è: {int(median_hour)}:00\")\n",
    "\n",
    "# Ordinare i valori per distanza dalla media\n",
    "df_total['distance_from_mean'] = np.abs(df_total['hour'] - mu_decimal)\n",
    "df_sorted = df_total.sort_values('distance_from_mean')\n",
    "\n",
    "# Plot della distribuzione con media e mediana\n",
    "xmin, xmax = 0, 23\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "gaussian_curve = norm.pdf(x, mu_decimal, sigma)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, gaussian_curve, 'r-', lw=2, label='Adattamento Gaussiano')  # Adattamento Gaussiano\n",
    "\n",
    "# Creare l'istogramma con binwidth=1 per ogni ora\n",
    "sns.histplot(df_sorted['hour'], binwidth=1, color='skyblue', edgecolor='black', stat='density', kde=False, alpha=0.6)\n",
    "\n",
    "# Linee per la media, mediana e deviazione standard\n",
    "plt.axvline(mu_decimal, color='red', linestyle='--', label=f'Media ({hours}:{minutes:02d})')\n",
    "plt.axvline(mu_decimal + sigma, color='green', linestyle='--', label='Media + 1σ')\n",
    "plt.axvline(mu_decimal - sigma, color='green', linestyle='--', label='Media - 1σ')\n",
    "\n",
    "# Titoli e legende\n",
    "plt.title(\"Distribuzione delle corse per ora con Media e Mediana\")\n",
    "plt.xlabel(\"Ora\")\n",
    "plt.ylabel(\"Densità\")\n",
    "plt.xticks(np.arange(0, 24, 1))\n",
    "plt.xlim(xmin, xmax)  # Imposta il limite dell'asse x per centrarlo correttamente\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualizzazione outliers con boxplot\n",
    "plt.figure(figsize=(10, 2))\n",
    "sns.boxplot(x=df_total['hour'], color='skyblue')\n",
    "plt.title(\"Distribuzione dei valori di Hour con Outliers\")\n",
    "plt.xlabel(\"Ora\")\n",
    "plt.xlim(xmin, xmax)  # Imposta il limite dell'asse x per centrarlo correttamente nel boxplot\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "colum = ['tpep_pickup_datetime'] # scelgo la colonna tpep_pickup_datetime\n",
    "df_2015_01_filtered = pd.read_csv(file_path1, usecols=colum)    # Load the dataset  in a DataFrame\n",
    "df_2016_01 = pd.read_csv(file_path2, usecols=colum)     \n",
    "df_2016_02 = pd.read_csv(file_path3, usecols=colum)\n",
    "df_2016_03 = pd.read_csv(file_path4, usecols=colum)\n",
    "\n",
    "df_total = pd.concat([df_2015_01_filtered, df_2016_01, df_2016_02, df_2016_03], ignore_index=True)\n",
    "\n",
    "# convert the column to datetime\n",
    "df_total['tpep_pickup_datetime'] = pd.to_datetime(df_total['tpep_pickup_datetime'])\n",
    "\n",
    "# extract the hour from the datetime\n",
    "df_total['hour'] = df_total['tpep_pickup_datetime'].dt.hour\n",
    "#calculate trip for hour\n",
    "hour_freq = df_total['hour'].value_counts().sort_index()\n",
    "#plot for hour distribution of trips\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=hour_freq.index, y=hour_freq.values, color='skyblue')\n",
    "plt.title(\"Distriubution of trips by hour\")\n",
    "plt.xlabel(\"hour\")\n",
    "plt.ylabel(\"Number of trips\")\n",
    "plt.xticks(np.arange(0, 24, 1))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analisys of trip distance (code 11)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#! before run this you must run the code 1 and 2\n",
    "\n",
    "plt.style.use('default')  \n",
    "sns.set_theme(style=\"white\")  # Set the style of the plots\n",
    "\n",
    "# Load the dataset in a DataFrame\n",
    "columns=['trip_distance']\n",
    "df_2015_01 = pd.read_csv(file_path1, usecols=columns)   # Load the dataset  in a DataFrame  \n",
    "df_2016_01 = pd.read_csv(file_path2, usecols=columns)\n",
    "df_2016_02 = pd.read_csv(file_path3, usecols=columns)\n",
    "df_2016_03 = pd.read_csv(file_path4, usecols=columns)\n",
    "\n",
    "# Filter date with trip distance between 0 and 100 miles\n",
    "df_total = pd.concat([df_2015_01, df_2016_01, df_2016_02, df_2016_03], ignore_index=True)\n",
    "df_total_filtered = df_total[(df_total['trip_distance'] > 0) & (df_total['trip_distance'] <= 40)]\n",
    "\n",
    "# Calculate the mean and standard deviation of the distribution\n",
    "mu, std = norm.fit(df_total_filtered['trip_distance'])\n",
    "\n",
    "# Create the Gaussian distribution\n",
    "x = np.linspace(0, 40, 80)\n",
    "p = norm.pdf(x, mu, std)        # Gaussian distribution\n",
    "\n",
    "# Plot the histogram and the Gaussian distribution\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.histplot(df_total_filtered['trip_distance'], bins=40, color='skyblue', edgecolor='black', stat='density', kde=False, alpha=0.6)\n",
    "plt.plot(x, p, 'r-', lw=2, label=f'Gaussian fit (μ={mu:.2f}, σ={std:.2f})')  # Curve of the Gaussian distribution\n",
    "plt.title(\"Distribuzione delle distanze dei viaggi (fino a 20 miglia)\")\n",
    "plt.xlabel(\"Distanza del viaggio (miglia)\")\n",
    "plt.ylabel(\"Densità\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the distribution of the trip duration by the hours (code 12)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#! before run this you must run the code 1, 2  and 3\n",
    "# #unite all dataset in a single csv\n",
    "file_paths=[file_path2, file_path3, file_path4]\n",
    "def load_columns(file_paths, columns_envolved):\n",
    "    df_list = []\n",
    "    for file_path in file_paths:\n",
    "        # load only the columns of interest\n",
    "        df = pd.read_csv(file_path, usecols=columns_envolved)\n",
    "        # correct the name of the column RateCodeID\n",
    "        # df = df[df['RatecodeID'].isin([1, 6])]\n",
    "        if 'RateCodeID' in df.columns:\n",
    "            df.rename(columns={'RateCodeID': 'RatecodeID'}, inplace=True)\n",
    "        df_list.append(df)\n",
    "    # unite all the dataframes\n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "df_distance = load_columns(file_paths, columns_envolved=['trip_distance'])\n",
    "df_time_fase = load_columns(file_paths, columns_envolved=['trip_distance', 'tpep_pickup_datetime'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 3. Cluster the data by the hour\n",
    "#df_ratecode_parzial = load_colums(file_paths, colums_envolved=['trip_distance', 'RatecodeID'])\n",
    "#filter ratecodeID between 0 and 6\n",
    "\n",
    "df_ratecode_parzial = load_columns(file_paths, columns_envolved=['trip_distance', 'RatecodeID', 'tpep_pickup_datetime'])\n",
    "df_2015_01_filtered = df_2015_01[['trip_distance', 'RatecodeID', 'tpep_pickup_datetime']]    # Load the dataset  in a DataFrame\n",
    "df_ratecode=pd.concat([df_2015_01_filtered, df_ratecode_parzial], ignore_index=True)\n",
    "# convert the column to datetime\n",
    "df_ratecode['tpep_pickup_datetime'] = pd.to_datetime(df_ratecode_parzial['tpep_pickup_datetime'])\n",
    "df_ratecode['pickup_hour'] = df_ratecode['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "# Calculate the number of trips for each hour\n",
    "trip_for_hour = df_ratecode.groupby('pickup_hour').size()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code 13"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#! before run this you must run the code 1, 2 ,3 and 12\n",
    "\n",
    "distance_media_for_hour = df_ratecode.groupby('pickup_hour')['trip_distance'].mean()\n",
    "\n",
    "# Plot the number of trips and the average distance for each hour\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Number of trips for each hour\n",
    "plt.subplot(2, 1, 1)\n",
    "trip_for_hour.plot(kind='bar', color='skyblue')\n",
    "plt.title('Numero di Viaggi per Ora')\n",
    "plt.xlabel('Ora del Giorno')\n",
    "plt.ylabel('Numero di Viaggi')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Distance for each hour\n",
    "plt.subplot(2, 1, 2)\n",
    "distance_media_for_hour.plot(kind='line', marker='o', color='orange')\n",
    "plt.title('Distanza Media per Ora')\n",
    "plt.xlabel('Ora del Giorno')\n",
    "plt.ylabel('Distanza Media del Viaggio')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crete relations with ratecode and trip distance (code 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map of the trips (code 15)\n",
    "# exercise with folium"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "columns_needed = [\n",
    "    'tpep_pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n",
    "    'dropoff_longitude', 'dropoff_latitude'\n",
    "]\n",
    "\n",
    "df_2015_01 = pd.read_csv(file_path1, usecols=columns_needed)   # Load the dataset  in a DataFrame\n",
    "df_2016_01 = pd.read_csv(file_path2, usecols=columns_needed)\n",
    "df_2016_02 = pd.read_csv(file_path3, usecols=columns_needed)\n",
    "df_2016_03 = pd.read_csv(file_path4, usecols=columns_needed)\n",
    "#concate all the dataframes\n",
    "df= pd.concat([df_2015_01, df_2016_01, df_2016_02, df_2016_03], ignore_index=True)\n",
    "\n",
    "#convert the column to datetime\n",
    "df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "\n",
    "# define the period of the day\n",
    "def get_hour_period(hour):\n",
    "    if 6 <= hour < 12:\n",
    "        return 'morning'\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'afternoon'\n",
    "    elif 18 <= hour < 24:\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "# create a column for the pickup period\n",
    "df['pickup_period'] = df['tpep_pickup_datetime'].dt.hour.apply(get_hour_period)\n",
    "\n",
    "# Filter the data only for the morning period\n",
    "df = df[(df['pickup_latitude'] != 0) & (df['pickup_longitude'] != 0) &\n",
    "        (df['dropoff_latitude'] != 0) & (df['dropoff_longitude'] != 0)]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create the map centered on New York City\n",
    "nyc_map = folium.Map(location=[40.7128, -74.0060], zoom_start=12)\n",
    "\n",
    "# Add a cluster to group the points\n",
    "marker_cluster = MarkerCluster().add_to(nyc_map)\n",
    "\n",
    "# Group the most frequent routes\n",
    "for period in df['pickup_period'].unique():\n",
    "    period_data = df[df['pickup_period'] == period]\n",
    "    #   Group the most frequent routes\n",
    "    frequent_routes = period_data.groupby(['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']).size().reset_index(name='counts')\n",
    "    top_routes = frequent_routes.nlargest(40, 'counts') \n",
    "    \n",
    "    # Add the routes, the start and end markers on the map\n",
    "    for _, row in top_routes.iterrows():\n",
    "        folium.PolyLine(\n",
    "            locations=[(row['pickup_latitude'], row['pickup_longitude']), (row['dropoff_latitude'], row['dropoff_longitude'])],\n",
    "            color='blue' if period == 'morning' else 'green' if period == 'afternoon' else 'orange' if period == 'evening' else 'purple',\n",
    "            weight=20,\n",
    "            opacity=1  \n",
    "        ).add_to(marker_cluster)\n",
    "\n",
    "\n",
    "# Show the map\n",
    "nyc_map"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another type of maps (code 16) Clusterizzazione per Tratte"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Filter only the data for the morning\n",
    "morning_data = df[df['pickup_period'] == 'morning']\n",
    "\n",
    "# create a map centered on New York City\n",
    "nyc_map = folium.Map(location=[40.7128, -74.0060], zoom_start=12)\n",
    "\n",
    "# add a cluster to group the points\n",
    "marker_cluster = MarkerCluster().add_to(nyc_map)\n",
    "\n",
    "# gruop the most frequent routes\n",
    "frequent_routes = morning_data.groupby(\n",
    "    ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']\n",
    ").size().reset_index(name='counts')\n",
    "top_routes = frequent_routes.nlargest(40, 'counts')  # Prendiamo le 40 tratte più comuni\n",
    "\n",
    "# add the routes, the start and end markers on the map\n",
    "for _, row in top_routes.iterrows():\n",
    "    # trip\n",
    "    folium.PolyLine(\n",
    "        locations=[\n",
    "            (row['pickup_latitude'], row['pickup_longitude']),\n",
    "            (row['dropoff_latitude'], row['dropoff_longitude'])\n",
    "        ],\n",
    "        color='blue',  # Colore per il tragitto\n",
    "        weight=5,      # Spessore della linea\n",
    "        opacity=0.7    # Opacità per il tragitto\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "    # Marker pickup\n",
    "    folium.Marker(\n",
    "        location=(row['pickup_latitude'], row['pickup_longitude']),\n",
    "        icon=folium.Icon(color='blue', icon='play', prefix='fa'),  # Icona per identificare il punto di partenza\n",
    "        popup='Partenza'\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "    # Marker dropoff\n",
    "    folium.Marker(\n",
    "        location=(row['dropoff_latitude'], row['dropoff_longitude']),\n",
    "        icon=folium.Icon(color='red', icon='flag', prefix='fa'),  # Icona per identificare il punto di arrivo\n",
    "        popup='Arrivo'\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Visualizza la mappa\n",
    "nyc_map\n",
    "# todo: problem whit association of the points"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization ten most pick up and drop off points (code 17)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Filter only the data for the morning\n",
    "morning_data = df[df['pickup_period'] == 'morning']\n",
    "\n",
    "# Find the 10 most frequent pickup points\n",
    "top_pickups = morning_data.groupby(['pickup_latitude', 'pickup_longitude']).size().reset_index(name='counts')\n",
    "top_pickups = top_pickups.nlargest(10, 'counts').sort_values(by='counts', ascending=False)  # Ordinati per frequenza\n",
    "\n",
    "# Find the 10 most frequent dropoff points\n",
    "top_dropoffs = morning_data.groupby(['dropoff_latitude', 'dropoff_longitude']).size().reset_index(name='counts')\n",
    "top_dropoffs = top_dropoffs.nlargest(10, 'counts').sort_values(by='counts', ascending=False)  # Ordinati per frequenza\n",
    "\n",
    "# Create the map\n",
    "nyc_map = folium.Map(location=[40.7128, -74.0060], zoom_start=12)\n",
    "\n",
    "# Cluster for pickup and dropoff points (is not important in ten most pick up and drop off points)\n",
    "pickup_cluster = MarkerCluster(name='Top 10 Pickup Points - Morning').add_to(nyc_map)\n",
    "dropoff_cluster = MarkerCluster(name='Top 10 Dropoff Points - Morning').add_to(nyc_map)\n",
    "\n",
    "# Add the pickup points with explicit index\n",
    "f=0\n",
    "for index, row in top_pickups.iterrows():\n",
    "    f += 1  # Incrementa f\n",
    "    folium.Marker(\n",
    "        location=(row['pickup_latitude'], row['pickup_longitude']),\n",
    "        icon=folium.Icon(color='blue', icon='play', prefix='fa'),\n",
    "        popup=f'Questo è il numero {f} di punti di partenza (frequenza: {row[\"counts\"]})'  # Indice corretto\n",
    "    ).add_to(pickup_cluster)\n",
    "\n",
    "# Add the dropoff points with explicit index\n",
    "k = 0  \n",
    "for index, row in top_dropoffs.iterrows():\n",
    "    k += 1  # increments k\n",
    "    folium.Marker(\n",
    "        location=(row['dropoff_latitude'], row['dropoff_longitude']),\n",
    "        icon=folium.Icon(color='red', icon='flag', prefix='fa'),\n",
    "        popup=f'Questo è il numero {k} di punti di arrivo (frequenza: {row[\"counts\"]})'  # Indice corretto\n",
    "    ).add_to(dropoff_cluster)\n",
    "\n",
    "# add a layer control\n",
    "folium.LayerControl().add_to(nyc_map)\n",
    "\n",
    "# show the map\n",
    "nyc_map\n",
    "#!!!!!! todo:  problem with the zoom of map"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering big data visualization (code 17)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Filter only the data for the morning\n",
    "morning_data = df[df['pickup_period'] == 'morning']\n",
    "\n",
    "# Find the 100 most frequent pickup points\n",
    "top_pickups = morning_data.groupby(['pickup_latitude', 'pickup_longitude']).size().reset_index(name='counts')\n",
    "top_pickups = top_pickups.nlargest(100, 'counts').sort_values(by='counts', ascending=False)  # Ordinati per frequenza\n",
    "\n",
    "# Find the 100 most frequent dropoff points\n",
    "top_dropoffs = morning_data.groupby(['dropoff_latitude', 'dropoff_longitude']).size().reset_index(name='counts')\n",
    "top_dropoffs = top_dropoffs.nlargest(100, 'counts').sort_values(by='counts', ascending=False)  # Ordinati per frequenza\n",
    "\n",
    "# Create the map\n",
    "nyc_map = folium.Map(location=[40.7128, -74.0060], zoom_start=12)\n",
    "\n",
    "# Cluster for pickup and dropoff points\n",
    "pickup_cluster = MarkerCluster(name='Top 10 Pickup Points - Morning').add_to(nyc_map)\n",
    "dropoff_cluster = MarkerCluster(name='Top 10 Dropoff Points - Morning').add_to(nyc_map)\n",
    "\n",
    "# Add the pickup points with explicit index\n",
    "for index, row in top_pickups.iterrows():\n",
    "    folium.Marker(\n",
    "        location=(row['pickup_latitude'], row['pickup_longitude']),\n",
    "        icon=folium.Icon(color='blue', icon='play', prefix='fa'),\n",
    "        popup=f'Punto di partenza #{index + 1} (frequenza: {row[\"counts\"]})'  # Indice corretto\n",
    "    ).add_to(pickup_cluster)\n",
    "\n",
    "# Add the dropoff points with explicit index\n",
    "k = 0 \n",
    "for index, row in top_dropoffs.iterrows():\n",
    "    k += 1  \n",
    "    folium.Marker(\n",
    "        location=(row['dropoff_latitude'], row['dropoff_longitude']),\n",
    "        icon=folium.Icon(color='red', icon='flag', prefix='fa'),\n",
    "        popup=f'Punto di arrivo #{k} (frequenza: {row[\"counts\"]})'  # Indice corretto\n",
    "    ).add_to(dropoff_cluster)\n",
    "\n",
    "# \n",
    "for _, pickup in top_pickups.iterrows():\n",
    "    for _, dropoff in top_dropoffs.iterrows():\n",
    "        # Check if the route is in the morning data\n",
    "        route_data = morning_data[\n",
    "            (morning_data['pickup_latitude'] == pickup['pickup_latitude']) &\n",
    "            (morning_data['pickup_longitude'] == pickup['pickup_longitude']) &\n",
    "            (morning_data['dropoff_latitude'] == dropoff['dropoff_latitude']) &\n",
    "            (morning_data['dropoff_longitude'] == dropoff['dropoff_longitude'])\n",
    "        ]\n",
    "        \n",
    "        if not route_data.empty:\n",
    "            folium.PolyLine(\n",
    "                locations=[(pickup['pickup_latitude'], pickup['pickup_longitude']),\n",
    "                           (dropoff['dropoff_latitude'], dropoff['dropoff_longitude'])],\n",
    "                color='green',\n",
    "                weight=2,\n",
    "                opacity=0.5\n",
    "            ).add_to(nyc_map)\n",
    "\n",
    "# Add a layer control\n",
    "folium.LayerControl().add_to(nyc_map)\n",
    "\n",
    "# Show the map\n",
    "nyc_map\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Correlation between the trip distance and the trip duration (code 18)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "colum = ['tpep_pickup_datetime']\n",
    "df_2015_01_filtered = pd.read_csv(file_path1, usecols=colum)\n",
    "df_2016_01 = pd.read_csv(file_path2, usecols=colum)\n",
    "df_2016_02 = pd.read_csv(file_path3, usecols=colum)\n",
    "df_2016_03 = pd.read_csv(file_path4, usecols=colum)\n",
    "df_total = pd.concat([df_2015_01_filtered, df_2016_01, df_2016_02, df_2016_03], ignore_index=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#  convert in datetime\n",
    "df_total['tpep_pickup_datetime'] = pd.to_datetime(df_total['tpep_pickup_datetime'])\n",
    "\n",
    "# extract the hour from the datetime\n",
    "df_total['hour'] = df_total['tpep_pickup_datetime'].dt.hour\n",
    "df_total['day_of_week'] = df_total['tpep_pickup_datetime'].dt.day_name()\n",
    "\n",
    "# Calculate the number of trips for each hour\n",
    "heatmap_data = df_total.groupby(['day_of_week', 'hour']).size().reset_index(name='trip_count')\n",
    "\n",
    "# Pivot the data for the heatmap\n",
    "heatmap_pivot = heatmap_data.pivot(index='day_of_week', columns='hour', values='trip_count')\n",
    "\n",
    "# Reorder the days of the week\n",
    "ordered_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "heatmap_pivot = heatmap_pivot.reindex(ordered_days)\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(30, 15))\n",
    "sns.heatmap(heatmap_pivot, cmap='YlGnBu', linewidths=0.5, annot=True, fmt='g')\n",
    "\n",
    "plt.title(\"Distribuzione media dei viaggi per ora e giorno della settimana\")\n",
    "plt.xlabel(\"Ora del giorno\")\n",
    "plt.ylabel(\"Giorno della settimana\")\n",
    "plt.show()# Calcola il numero totale di corse per giorno della settimana\n",
    "\n",
    "\n",
    "daily_trip_totals = heatmap_data.groupby('day_of_week')['trip_count'].sum()\n",
    "\n",
    "# Calcola la percentuale di corse per ogni ora rispetto all'intera giornata\n",
    "heatmap_data['hourly_percentage'] = heatmap_data.apply(\n",
    "    lambda row: (row['trip_count'] / daily_trip_totals[row['day_of_week']]) * 100, axis=1\n",
    ")\n",
    "\n",
    "# Pivota i dati per la heatmap delle percentuali\n",
    "heatmap_pivot_percentage = heatmap_data.pivot(index='day_of_week', columns='hour', values='hourly_percentage')\n",
    "heatmap_pivot_percentage = heatmap_pivot_percentage.reindex(ordered_days)\n",
    "\n",
    "# Heatmap con le percentuali\n",
    "plt.figure(figsize=(30, 15))\n",
    "sns.heatmap(heatmap_pivot_percentage, cmap='YlGnBu', linewidths=0.5, annot=True, fmt='.2f', cbar_kws={'label': '% di corse rispetto all’intera giornata'})\n",
    "\n",
    "plt.title(\"Distribuzione percentuale delle corse per ora e giorno della settimana\")\n",
    "plt.xlabel(\"Ora del giorno\")\n",
    "plt.ylabel(\"Giorno della settimana\")\n",
    "plt.show()\n",
    "\n",
    "# Grafico a barre con il numero totale di corse per ogni giorno della settimana\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=daily_trip_totals.index, y=daily_trip_totals.values, palette='muted')\n",
    "\n",
    "plt.title(\"Totale corse per giorno della settimana\")\n",
    "plt.xlabel(\"Giorno della settimana\")\n",
    "plt.ylabel(\"Numero di corse\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convertiamo la colonna in datetime\n",
    "df_total['tpep_pickup_datetime'] = pd.to_datetime(df_total['tpep_pickup_datetime'])\n",
    "\n",
    "# Estraiamo l'ora e il giorno della settimana\n",
    "df_total['hour'] = df_total['tpep_pickup_datetime'].dt.hour\n",
    "df_total['day_of_week'] = df_total['tpep_pickup_datetime'].dt.day_name()\n",
    "\n",
    "# Calcoliamo il numero di corse per ogni ora\n",
    "heatmap_data = df_total.groupby(['day_of_week', 'hour']).size().reset_index(name='trip_count')\n",
    "\n",
    "# Riordiniamo i giorni della settimana\n",
    "ordered_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "heatmap_data['day_of_week'] = pd.Categorical(heatmap_data['day_of_week'], categories=ordered_days, ordered=True)\n",
    "\n",
    "# Pivotiamo i dati per la heatmap\n",
    "heatmap_pivot = heatmap_data.pivot(index='day_of_week', columns='hour', values='trip_count')\n",
    "\n",
    "# Heatmap del numero di corse per ora\n",
    "plt.figure(figsize=(30, 15))\n",
    "sns.heatmap(heatmap_pivot, cmap='YlGnBu', linewidths=0.5, annot=True, fmt='g')\n",
    "\n",
    "plt.title(\"Distribuzione media dei viaggi per ora e giorno della settimana\")\n",
    "plt.xlabel(\"Ora del giorno\")\n",
    "plt.ylabel(\"Giorno della settimana\")\n",
    "plt.show()\n",
    "\n",
    "# Calcoliamo il numero totale di corse per ogni giorno della settimana\n",
    "daily_trip_totals = heatmap_data.groupby('day_of_week')['trip_count'].sum()\n",
    "\n",
    "# Calcoliamo la percentuale di corse per ogni ora rispetto all'intera giornata\n",
    "heatmap_data['hourly_percentage'] = heatmap_data.apply(\n",
    "    lambda row: (row['trip_count'] / daily_trip_totals[row['day_of_week']]) * 100, axis=1\n",
    ")\n",
    "\n",
    "# Pivota i dati per la heatmap delle percentuali\n",
    "heatmap_pivot_percentage = heatmap_data.pivot(index='day_of_week', columns='hour', values='hourly_percentage')\n",
    "heatmap_pivot_percentage = heatmap_pivot_percentage.reindex(ordered_days)\n",
    "\n",
    "# Heatmap delle percentuali\n",
    "plt.figure(figsize=(30, 15))\n",
    "sns.heatmap(heatmap_pivot_percentage, cmap='YlGnBu', linewidths=0.5, annot=True, fmt='.2f', cbar_kws={'label': '% di corse rispetto all’intera giornata'})\n",
    "\n",
    "plt.title(\"Distribuzione percentuale delle corse per ora e giorno della settimana\")\n",
    "plt.xlabel(\"Ora del giorno\")\n",
    "plt.ylabel(\"Giorno della settimana\")\n",
    "plt.show()\n",
    "\n",
    "# Grafico a barre con il numero totale di corse per ogni giorno della settimana\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=daily_trip_totals.index, y=daily_trip_totals.values, palette='muted')\n",
    "\n",
    "plt.title(\"Totale corse per giorno della settimana\")\n",
    "plt.xlabel(\"Giorno della settimana\")\n",
    "plt.ylabel(\"Numero di corse\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Correlation of Pearson and Spearman (code 19)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#!!!!!!!!! I Use only the 2016_01 file, i mus use all file, but the code is the same and the weight of the file is too big\n",
    "# colum = ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'RatecodeID', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount']\n",
    "# colum = ['VendorID','Passenger_count']\n",
    "\n",
    "# Nick con l'm2 prova a far girare i codici qui sotto \n",
    "df_2016_01 = pd.read_csv(file_path2)\n",
    "df_2016_02 = pd.read_csv(file_path3)\n",
    "df_2016_03 = pd.read_csv(file_path4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_total = pd.concat([df_2015_01, df_2016_01, df_2016_02, df_2016_03], ignore_index=True)\n",
    "df_numeric = df_total.select_dtypes(include=[np.number])\n",
    "\n",
    "# 'VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'RateCodeID', 'store_and_fwd_flag', 'dropoff_longitude', 'dropoff_latitude', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Filter for gestion of NaN\n",
    "\n",
    "df_numeric = df_numeric.dropna()  #re    Smove the NaN values\n",
    "\n",
    "# Correlation of Pearson\n",
    "pearson_corr = df_numeric.corr(method='pearson')\n",
    "\n",
    "# Correlation of Spearman\n",
    "# spearman_corr = df_numeric.corr(method='spearman')\n",
    "\n",
    "# Heatmap per Pearson\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pearson_corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Heatmap Correlazione di Pearson\")\n",
    "plt.show()\n",
    "\n",
    "# Heatmap per Spearman\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.heatmap(spearman_corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "# plt.title(\"Heatmap Correlazione di Spearman\")\n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "spearman_corr = df_numeric.corr(method='spearman')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(spearman_corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Heatmap Correlazione di Spearman\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Correlation of Cramer's V (code 20)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "\n",
    "#Definizion of cramers_v\n",
    "def cramers_v(confusion_matrix):\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.values.sum()  #  Total number of samples\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
    "    rcorr = r - ((r - 1) ** 2) / (n - 1)\n",
    "    kcorr = k - ((k - 1) ** 2) / (n - 1)\n",
    "    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
    "\n",
    "\n",
    "# select the categorical columns\n",
    "categorical_cols = ['RatecodeID', 'payment_type', 'VendorID']  # adapt the columns to the dataset\n",
    "\n",
    "# drop the NaN values\n",
    "df_cleaned = df_2016_01[categorical_cols].dropna()\n",
    "\n",
    "# Calculate the Cramer's V correlation for each pair of categorical columns\n",
    "results_cramers = {}\n",
    "for col1 in categorical_cols:\n",
    "    for col2 in categorical_cols:\n",
    "        if col1 != col2:  # Avoid calculating the correlation of a column with itself\n",
    "            confusion_matrix = pd.crosstab(df_cleaned[col1], df_cleaned[col2])\n",
    "            value = cramers_v(confusion_matrix)\n",
    "            results_cramers[(col1, col2)] = value\n",
    "\n",
    "# Create a matrix with the results\n",
    "cramers_matrix = pd.DataFrame(index=categorical_cols, columns=categorical_cols, dtype=float)\n",
    "\n",
    "for (col1, col2), value in results_cramers.items():\n",
    "    cramers_matrix.loc[col1, col2] = value\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cramers_matrix.astype(float), annot=True, cmap=\"coolwarm\", cbar=True,\n",
    "            xticklabels=categorical_cols, yticklabels=categorical_cols, vmin=0, vmax=1)\n",
    "plt.title(\"Cramer's V Correlations Heatmap\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analysis of the negative distribution for tips (code 21)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "colum = ['payment_type', 'tip_amount', 'total_amount']\n",
    "df_2015_01 = pd.read_csv(file_path1, usecols=colum)\n",
    "df_2016_01 = pd.read_csv(file_path2, usecols=colum)\n",
    "df_2016_02 = pd.read_csv(file_path3, usecols=colum)\n",
    "df_2016_03 = pd.read_csv(file_path4, usecols=colum)\n",
    "#concat all the dataframes\n",
    "df_total = pd.concat([df_2015_01, df_2016_01, df_2016_02, df_2016_03], ignore_index=True)\n",
    "\n",
    "payment_mapping = {\n",
    "    1: \"Credit card\",\n",
    "    2: \"Cash\"\n",
    "}\n",
    "\n",
    "# Filtro per i tipi di pagamento specificati (1: Credit card, 2: Cash)\n",
    "df_total['payment_type'] = df_total['payment_type'].map(payment_mapping)\n",
    "filtered_data = df_total[df_total['payment_type'].isin(['Credit card', 'Cash'])]\n",
    "\n",
    "# Gruppo per tipo di pagamento\n",
    "summary = filtered_data.groupby('payment_type').agg(\n",
    "    total_tip_amount=('tip_amount', 'sum'),\n",
    "    total_amount=('total_amount', 'sum')\n",
    ")\n",
    "\n",
    "# Calcolo percentuale delle mance\n",
    "summary['tip_percentage'] = (summary['total_tip_amount'] / summary['total_amount']) * 100\n",
    "\n",
    "# Stampa il risultato\n",
    "print(summary)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Grafico a Barre Sovrapposte\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "summary_sorted = summary.sort_values('total_tip_amount', ascending=False)\n",
    "\n",
    "# Aggiunta delle barre\n",
    "sns.barplot(x='total_tip_amount', y=summary_sorted.index, data=summary_sorted, color='blue', label='Total Tip Amount')\n",
    "sns.barplot(x=summary_sorted['total_amount'] * (summary_sorted['tip_percentage'] / 100), \n",
    "            y=summary_sorted.index, data=summary_sorted, color='orange', label='Total Amount w/ Tip (%)')\n",
    "\n",
    "# Aggiunta di una legenda\n",
    "plt.title('Total Tips and Tip Percentage Contribution by Payment Type')\n",
    "ax.set_xlabel('Dollars ($)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Grafico con Percentuali in Linea e Importi\n",
    "fig, axes = plt.subplots(nrows=2, figsize=(12, 8))\n",
    "\n",
    "sns.barplot(data=summary_sorted, y=summary_sorted.index, x='total_tip_amount', ax=axes[0], palette='Blues')\n",
    "sns.barplot(data=summary_sorted, y=summary_sorted.index, x='tip_percentage', ax=axes[1], palette='Reds')\n",
    "\n",
    "axes[0].set_title('Tip Totals per Payment Type')\n",
    "axes[1].set_title('Percentage of Tip per Total Fare')\n",
    "\n",
    "axes[1].set(xlabel='Percentage(%)')\n",
    "axes[0].set(xlabel='Amount($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
